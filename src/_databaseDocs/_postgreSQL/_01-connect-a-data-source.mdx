import BaseUrlLink from '@site/src/_databaseDocs/baseLink';

## What's about to happen?

We want to connect our PostgreSQL database to our API. To do this, we use the Hasura PostgreSQL data connector to
facilitate the connection and then introspect the database to generate JSON which the Hasura CLI will then use to create
metadata which can then define your API.

## Step 1. Init the PostgreSQL connector

:::tip Required

- <BaseUrlLink to="/getting-started/prerequisites" text="DDN CLI" />
- A new or existing <BaseUrlLink to="/getting-started/init-supergraph" text="supergraph" />
- A new or existing <BaseUrlLink to="/getting-started/init-subgraph" text="subgraph" />

:::

To initialize the PostgreSQL connector, run the following:

```bash
ddn connector init my_pg  --subgraph my_subgraph  --hub-connector hasura/postgres
```

In this command, we're passing a few important values.

**Connector name**

We're naming the connector `my_pg` in this example, but you can call it whatever makes sense to you. For example, if
this connector is integrating a database all about product metadata, it would make sense to name it something like
`product_metadata_postgres_connector`.

**Directory for the connector**

We want to keep our project organized with each data connector's configuration located in a relevant subgraph directory.
To do this we're passing the required `--dir` flag to tell the CLI that the configuration files it's generating should
be saved in the subgraph directory. In this example the CLI will create a `connector/my_pg` directory if it doesn't
exist.

:::note Best practice

Importantly, a data connector can only connect to one data source.

The name of the connector and the directory in which the configuration is stored, `my_pg` in this example, should match
for convenience and clarity sake.

:::

**Connector type**

We're specifying that this connector should be the: `hasura/my_pg`, connector listed in the
[Connector Hub](https://hasura.io/connectors/postgres).

### What did this do?

In the `my_subgraph/connector/my_pg` directory which we specified in the command, the CLI created:

- A `connector.yaml` file which contains the configuration for the connector.
- A `connector-metadata.yaml` file which contains extra metadata for the connector.
- A `docker-compose.my_pg.yaml` a file to run the MongoDB data connector locally in Docker.
- A placeholder `.buildignore` file to prevent unnecessary files from being included in the build.
- An `.env.local` placeholder file for environment variables for pertaining to running this connector locally.

Right now, the CLI has only scaffolded out configuration files for the data connector. Our connector still knows nothing
about the PostgreSQL database or the data it contains. That's coming up in the next steps.

## Step 2. Modify the port

Typically, connectors default to port `8080`. Each time you add a connector, we recommend incrementing the published
port by one to avoid port collisions. For example:

Change the value of your `my_pg` connector's published port in your
`my_subgraph/connector/my_pg/docker-compose.my_pg.yaml` to `8082` to avoid port collisions with other connectors:

```yaml
ports:
  - mode: ingress
    target: 8080
    #highlight-start
    published: '8082'
    #highlight-end
    protocol: tcp
```

:::tip Consistency is important

In subsequent steps, when running your connector locally, it's critical to ensure this port value matches the connection
string you provide in your subgraph's `.env.my_subgraph` file.

:::

## Step 3. Add the connection URI

Now that our connector has been scaffolded out for us, we need to provide a connection string so that the data source
can be introspected and the boilerplate configuration can be taken care of by the CLI. Below, note that there are a few
caveats to consider before continuing.

The CLI has provided an `.env.local` file for our connector in the `my_subgraph/connector/my_pg` directory. We can
add a key-value pair of `CONNECTION_URI` along with the connection string itself to this file and our connector will use
this to connect to our PostgreSQL database. The file, after adding the `CONNECTION_URI` should look like this example:

```env
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://local.hasura.dev:4317
OTEL_SERVICE_NAME=my_subgraph_my_pg
#highlight-start
CONNECTION_URI=<postgres-connection-uri>
#highlight-end
```

:::tip Don't have a PostgreSQL database?

We've got you covered! You can use this read-only PostgreSQL databased hotsed on GCP.

```text
postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app
```

**Note: As this is read-only, you won't be able to complete the later section on mutating data.**

:::

You can use a local PostgreSQL database or a cloud-hosted option. If you already have one you can connect to, you can go
ahead and do that using the steps above. Hasura DDN will not modify your database in any way, so you can use an existing
database without any worries.

:::info Environment-specific caveats

**Local PostgreSQL**

If you're using a local PostgreSQL database — such as through [Docker](https://hub.docker.com/_/postgres) — you can
connect to it directly from the data connector. However, if you deploy your connector to Hasura DDN, the cloud-hosted
version of your data connector won't be able to find your database. You'll need to use a tool like
[ngrok](https://ngrok.com/) to tunnel your database's connection. This will expose the port, most likely `5432`, on
which the database is running and allow Hasura DDN to connect to it.

**Cloud-hosted PostgreSQL**

Alternatively, if you have a cloud-hosted database, as Hasura DDN will need to reach your database, ensure you've
allowlisted `0.0.0.0/0` so that DDN is able to reach it.

:::

## Step 4. Introspect your database

With the connector configured, we can now use the CLI to introspect our PostgreSQL database and create a source-specific
configuration file for our connector. In a new tab, run:

```bash
ddn connector introspect --connector my_subgraph/connector/my_pg/connector.yaml
```

If you look at the `configuration.json` for your connector, you'll see metadata describing your PostgreSQL schema.

:::tip Initialize a Git repository

At this point, we recommend initializing a Git repository. This gives you a fallback point as you begin to iterate on
your project.

:::

## What did this do?

The commands above introspected your data source to create a JSON configuration file.

## Next steps

<!-- prettier-ignore -->
With this JSON configuration, which represents the tables in PostgreSQL, we can now use it to <BaseUrlLink to="/getting-started/connect-to-data/create-source-metadata" text="generate Hasura metadata" />.
