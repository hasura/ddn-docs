---
sidebar_position: 2
sidebar_label: Getting Started
description: "Learn how to implement Hasura with a PostgreSQL data source, from start to finish."
keywords:
  - hasura
  - postgresql
  - data connector
  - graphql
  - queries
  - sql data types
  - graphql types
  - api
  - database
  - getting started
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Getting Started with Hasura and PostgreSQL

## Model evolution

Historically, iterating on APIs has presented challenges. Notably, versioning standards and schema evolution are not
handled elegantly and can lead to downstream disruptions for consumers. With Hasura, iterating on your API is as simple
as updating your metadata. We'll take a look at two common scenarios: adding a new field and deprecating an existing
one.

### Add a new field

As a model is supported by an underlying entity in your PostgreSQL database, you'll need to modify that first. If, for
instance, you add a column â€” such as `last_online` to a table â€” you can then update your `DataConnectorLink` by running
the following:

```bash
ddn connector-link update mypg
```

This will instantly introspect your data source and update the connector's configuration to reflect the new `lastOnline`
field. If this is part of our `Users` model, we'd then run the following to update the metadata:

```bash
ddn model update Users
```

If you have no plans to deprecate an existing field, you can now generate a new supergraph build.

### Deprecate an existing field

With a new field taking its place, we can now deprecate our existing `lastSeen` field by adding the `deprecated` key and
â€” optionally â€” providing a reason that will be passed along during schema introspection.

In the file whose field we want to deprecate, find the field and add the following in the `ObjectType` definition:

```yaml
---
kind: ObjectType
version: v1
definition:
  name: Users
  fields:
    # Other fields above...
    - name: lastSeen
      type: Timestamptz
      # highlight-start
      deprecated:
        reason: Use lastOnline instead.
      # highlight-end
```

If you now create a new build and try to run a query that includes this deprecated field, you'll receive a warning
alerting you to the field's deprecation:

<!-- TODO: Add screenshot -->

## Transformations

As we alluded to in the beginning of this guide, the remaining work on top of your existing PostgreSQL modeling is
authoring and maintaining custom code, or business logic. With Hasura, you can integrate â€” and even host â€” this business
logic directly with Hasura DDN and your API.

Hasura handles business logic using the TypeScript connector. This enables you to author you own custom code, written in
TypeScript, and host it alongside your API. Using this connector, you can transform and enrich data before it reaches
your consumers, unlocking you to simplify client applications and speed up your backend development.

You can then integrate this custom code as individual [**commands**](/supergraph-modeling/commands.mdx) in your
metadata.

### Step 1. Add the TypeScript connector

Let's begin by adding the connector to our project. In the example below, you'll see some familiar flags with new values
being passed to them. We'll call this the `ts_connector` and use the `hasura/nodejs` connector from the connector hub:

```bash
ddn connector init myts --dir <subgraph-name>/connector/myts --hub-connector hasura/nodejs
```

This will create the following directory structure in your subgraph's connector directory's new `myts` directory, with
the `functions.ts` file being your connector's entrypoint:

```bash
â”œâ”€â”€ connector.yaml
â”œâ”€â”€ docker-compose.myts.yaml
#highlight-start
â”œâ”€â”€ functions.ts
#highlight-end
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

### Step 2. Modify the connector's published port

Like with our PostgreSQL connector, to avoid port collisions, we'll change the connector's published port to `8082`. In
the `docker-compose.myts.yaml` file, which can be found in the directory you created in the last step, replace the
existing `published` value with:

```yaml
ports:
  - mode: ingress
    target: 8080
    #highlight-start
    published: "8082"
    #highlight-end
    protocol: tcp
```

Then, add the port's value to the `.env/.local` in your `myts` directory:

```env
HASURA_CONNECTOR_PORT=8082
```

### Step 3. Install dependencies

Within our `myts` directory, let's install any necessary dependencies:

```bash
cd <subgraph>/connector/myts && npm i
```

Then, from the `myts` directory run this command to start and watch the connector for any changes:

```bash
env $(cat .env.local | xargs) npm run watch
```

### Step 4. Add the DataConnectorLink

As with our PostgreSQL connector, and any connector, we'll now need to create the `DataConnectorLink` which will
translate our TypeScript functions into commands that can be exposed as queries and mutations via our GraphQL API.
Create this using:

```bash
ddn connector-link add myts
```

Then, update the `.env` values in your subgraph's env file to include this connector. The complete file should now look
like this:

```env
UX_MYPG_READ_URL=http://local-dev.hasura.me:8081
UX_MYPG_WRITE_URL=http://local-dev.hasura.me:8081
UX_MYTS_READ_URL=http://local-dev.hasura.me:8082
UX_MYTS_WRITE_URL=http://local-dev.hasura.me:8082
```

### Step 5. Write business logic

In our example, we're going to transform the timestamp returned on our `createdAt` field from `Users` into a
human-readable format. We'll replace the default `hello()` function in our `functions.ts` file with the following:

```ts
import { Client } from "pg";
import { config } from "dotenv";

config();

/**
 * @readonly
 */
export async function humanReadableCreatedAt(userId: string): Promise<string> {
  const client = new Client({
    connectionString: process.env.PG_URI,
  });

  await client.connect();

  const queryText = "SELECT created_at FROM users WHERE id = $1";
  const result = await client.query(queryText, [userId]);

  if (result.rows.length > 0) {
    const timestamp = result.rows[0].created_at;
    const date = new Date(timestamp);
    return date.toLocaleString();
  } else {
    return `Issue retrieving user`;
  }
}
```

:::tip Install any dependency and use environment varialbes

As this is a Node.js project, you can install any dependency! You can see we've installed the `dotenv` and `pg` packages
and are using the latter to interact directly with our PostgreSQL data source.

You'll also notice that we're using `process.env.PG_URI`, which references the `.env.local` file in the `myts`
subdirectory. You can add any values here you'd like and reference them in your functions.

:::

### Step 6. Track the new function

To add our function, similar to how we added our individual tables earlier, we can use the following to generate the
related Hasura metadata:

```bash
ddn connector-link update myts --add-all-resources
```

### Step 7. Create a new build and test

Next, let's create a new build of our supergraph:

```bash
ddn supergraph build local --output-dir ./engine
```

You should see your command available, along with its documentation, in the GraphiQL explorer:

<!-- TODO: Add screenshot of console -->

### Step 8. Create a relationship via a model

As it stands, our `humanReadableCreatedAt` command isn't super useful. It's preferable to return this value when we're
querying for a user's information. We can easily utilize this business logic in a request from a client by creating a
relationship.

For our example, let's take our `Users.hml` file and create a new relationship. Remember, we can use the LSP to help us
in authoring this:

```yaml
---
kind: Relationship
version: v1
definition:
  name: humanReadableCreatedAt
  source: Users
  target:
    command:
      name: HumanReadableCreatedAt
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        argument:
          argumentName: userId
```

We can then update our `Users` model:

```bash
ddn model update Users
```

And create a new build before testing this:

```bash
ddn-staging supergraph build local --output-dir ./engine
```

In our example, when querying a user's information, we can now return the transformed data as part of the simple query
from the client ðŸŽ‰

<!-- TODO: Add screenshot -->

## Authorization

Powerful authorization rules can be created simply using the same declarative metadata approach we've seen for
relationships and versioning. By adding
[`modelPermissions`](/supergraph-modeling/permissions.mdx#modelpermissions-modelpermissions) and
[`typePermissions`](/supergraph-modeling/permissions.mdx#typepermissions-typepermissions), you can easily control which
rows and columns from PostgreSQL are returned via your API.

### Step 1. Create a modelPermission

By default, a role of `admin` exists for your API and can access all models. To create a new role, such as `user`,
simply add the role to the list of `permissions` for a model and set up your access control rules. In the example below,
we'll allow users with the role of `user` to access only their own rows of data by checking for a header value matching
their `id`:

```yaml
# Users.hml
---
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    - role: admin
      select:
        filter: null
        #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
        #highlight-end
```

### Step 2. Create a typePermission

The `admin` role also has access to all fields for each model. However, adding a new role and limiting what columns can
be returned from your data source is just as simple. For example, let's restrict what a `user` can see by omitting
several fields from the `typePermissions`:

```yaml
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-start
    - role: user
      output:
        allowedFields:
          - email
          - favoriteArtist
          - id
          - name
          - password
    #highlight-end
```

Creating a new build after modifying this metadata will allow us to use two headers in our request: `x-hasura-role` and
`x-hasura-user-id`. In this example, we're passing them as unencoded values, but you can make these part of your claims
when utilizing [JWTs](/auth/authentication/jwt.mdx) or [webhooks](/auth/authentication/webhook.mdx) for authentication:

<!-- TODO: Add screenshot from console -->

:::tip Don't forget about LSP!

Remember, the LSP will help you author any metadata objects or updates, including `modelPermissions` and
`typePermissions`.

:::

## Writes and transactions

You can easily write data to your database using the native PostgreSQL driver and the TypeScript connector. In the
example below, we've truncated the code used in this section, but you follow these steps to create a function which
manipulates or inserts data and is exposed as a mutation via your GraphQL API.

### Step 1. Create a function

In the earlier example, we marked our `humanReadableCreatedAt()` function as `@readOnly` using JSDoc to expose it as a
query. However, omitting this value will expose the function as a mutation. We can even add documentation that will be
visible in the GraphiQL explorer by tagging `@param` for the argument(s) and `@returns` for what will be returned by the
mutation.

```ts
import { Client } from "pg";
import { config } from "dotenv";

config();

/**
 * @param userData An object containing the user's data.
 * @returns The ID of the newly inserted user.
 */
export async function insertUser(userData: { name: string; email: string }): Promise<string> {
  const client = new Client({
    connectionString: process.env.PG_URI,
  });

  await client.connect();

  const queryText = `
    INSERT INTO users (name, email)
    VALUES ($1, $2)
    RETURNING id
  `;
  const values = [userData.name, userData.email];
  const result = await client.query(queryText, values);

  await client.end();

  if (result.rows.length > 0) {
    return result.rows[0].id;
  } else {
    throw new Error("Failed to insert user");
  }
}
```

### Step 2. Track the function

Just as before, we'll track our function using the CLI by first updating our `DataConnectorLink`:

```bash
ddn connector-link update myts
```

And then bringing in the new command:

```bash
ddn command add --connector-link myts --name insertUser
```

### Step 3. Create a new build and test

Next, let's create a new build of our supergraph:

```bash
ddn supergraph build local --output-dir ./engine
```

Finally, should see your command available, along with its documentation, in the GraphiQL explorer as a mutation:

<!-- TODO: Add screenshot of console -->

## Decoupling your domain and physical data models

Native Queries allow you to run custom SQL queries on your PostgreSQL database. This allows you to run queries that are
not supported by Hasura DDN's GraphQL engine. This unlocks the full power of your database, allowing you to run complex
queries, mutations, and even vector searches â€” all directly from your Hasura GraphQL API.

In modern data architecture, decoupling the data domain from physical data models is a critical principle. This approach
promotes flexibility and scalability, allowing the logical data representation to evolve independently of the underlying
physical storage. By using Native Queries, you can define complex business logic and data transformations in SQL while
exposing a clean and simplified GraphQL API. This separation ensures that changes in the database schema or physical
data models do not disrupt the application layer, fostering a more maintainable and adaptable system architecture.

You can read more about these [here](/connectors/postgresql/native-queries/index.mdx).
