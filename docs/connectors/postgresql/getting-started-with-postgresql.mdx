---
sidebar_position: 2
sidebar_label: Getting Started
description: "Learn how to implement Hasura with a PostgreSQL data source, from start to finish."
keywords:
  - hasura
  - postgresql
  - data connector
  - graphql
  - queries
  - sql data types
  - graphql types
  - api
  - database
  - getting started
---

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

# Getting Started with Hasura and PostgreSQL

:::info Completing this guide

This guide will take approximately 30 minutes. By the end, you'll walk away with an understanding of:

- How the Hasura API authoring lifecycle works.
- How to connect a PostgreSQL database.
- How to run and develop with Hasura locally.
- How to use the new Hasura DDN CLI to create, iterate on, and deploy a supergraph to Hasura DDN.

:::

## Introduction

In this guide, you'll learn how you can build an API on PostgreSQL using Hasura DDN. As an experienced engineer, when
building APIs on PostgreSQL, you understand that 50-90% of your API is already implicitly defined by the modeling work
you've done within PostgreSQL. The remaining work â€” business logic â€” is what requires you to maintain custom code.

Traditional API frameworks solve this by requiring you to write code and use ORMs to abstract away the database, leading
to performance debt down the road. Hasura provides a revolutionary way of creating APIs on PostgreSQL that ensures the
highest levels of performance upfront. Hasura helps you automatically maintain an API for parts of your API that can
already be defined by the existing modeling work in PostgreSQL, and offers a lightweight and high-performance way to
write business logic wherever required.

Our approach is based on two key principles:

1. **Query compilation that is native to the database with push-down**: This principle ensures that queries are
   optimized and executed directly within the database, minimizing overhead and maximizing performance.

2. **Lazy evaluationâ€”only compute what is needed**: By evaluating only the necessary computations, Hasura minimizes
   resource usage and enhances performance efficiency.

This results in an API that your consumers will love because of its standardization and on-demand composability. When
leveraging Hasura's capabilities, your API will approach theoretical optimal performance, as if you wrote a custom,
optimized query for every single API endpoint ðŸš€

## API authoring lifecycle

The API authoring lifecycle with Hasura involves the following steps:

1. **Definition by models and commands in Hasura metadata**: The API is defined through models and commands that are
   stored within Hasura's metadata.

2. **Backing by database entities, code, or queries**: These models and commands are backed by the underlying database
   entities, custom code, or native queries.

3. **CLI tooling for easy authoring**: Hasura's CLI makes it easy to author models and commands by introspecting the
   database, code, or native queries.

By following these steps, you can efficiently create and maintain high-performance APIs on PostgreSQL with Hasura DDN.

## Modeling basics

Designing your database to align with your API model is essential for creating efficient and scalable applications. By
working backwards from the desired API, you can structure your database to optimize for read operations, which is often
the primary use case for most consumers. This approach works well for greenfield applications, but existing databases
can also be adapted using techniques like [views](https://www.postgresql.org/docs/current/tutorial-views.html).

### Start with the API

1. **Define the API**: Outline the resources, data structures, and relationships needed by your application.
2. **Design the Database**: Create tables, columns, and relationships that map directly to the API model.

### Focus on Reads

- **Optimize for queries**: Design your database to efficiently handle read operations, as they are typically the most
  frequent.
- **Use indexes**: Ensure that columns used in `WHERE`, `JOIN`, and `ORDER BY` clauses are indexed to improve
  performance.

## Prerequisites

Before continuing, ensure you've completed the following prerequisites:

### Step 1. Install the Hasura CLI

You can download the CLI binary below and start using it immediately. Please follow the instructions for your system.

<Tabs groupId="os-preference" className="api-tabs">

<TabItem value="macOS-or-linux" label="macOS and Linux">

Simply run the installer script in your terminal:

```bash
curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/v1/get.sh | bash
```

</TabItem>

<TabItem value="windows" label="Windows">

Download the latest `cli-ddn-windows-amd64.exe` binary and run it.

```bash
curl -L https://graphql-engine-cdn.hasura.io/ddn/cli/v1/latest/cli-ddn-windows-amd64.exe -o ddn.exe
```

:::info Unrecognized application warning

In Windows, if you get an "Unrecognized application" warning, click "Run anyway".

:::

</TabItem>
</Tabs>

### Step 2. Install the Hasura VS Code extension

If you don't already have [VS Code](https://code.visualstudio.com/download) installed, install it and then install the
[Hasura extension](https://marketplace.visualstudio.com/items?itemName=HasuraHQ.hasura).

### Step 3. Install Docker

Finally, install [Docker](https://docs.docker.com/engine/install/), which will be used for local development.

## Create a project

[**Projects**](/project-configuration/projects.mdx) contain all the configuration and metadata for your supergraph. A
project will contain exactly one supergraph. When you create a project, you're creating a resource on Hasura DDN.
Eventually, this can be used to serve your API. To create a project, run the following command:

```bash
ddn project create
```

The CLI will return the project name and console URL for your project. However, without completing any of the steps
below, a project has no value.

## Create a subgraph

[**Subgraphs**](/project-configuration/subgraphs.mdx) are mapped to specific data domains within an organization. For
example, if the PostgreSQL database which we'll be connecting in the next step is for customer profile data, it's a safe
bet that the team responsible for this â€” and other data sources â€” is our User Experience team. As such, we could create
a `ux` subgraph that will integrate this database (and possibly others) into our API using this command with the project
name from the previous step:

```bash
ddn project subgraph create <desired-subgraph-name> --project <project-name>
```

The CLI will respond with your new subgraph's name.

## Init your supergraph

With a single subgraph created, we can initialize our [**supergraph**](/project-configuration/config.mdx#supergraph). A
supergraph is equivalent to an API, and these terms can be used interchangeably. Your supergraph is the composite of all
your subgraphs and their various data sources, including custom business logic.

To init your supergraph, from an empty directory, run:

```bash
ddn supergraph init --dir . --with-project <project-name>
```

<details>
  <summary>
    The CLI will scaffold out all the files needed for local development and iteration, including a docker-compose. Click here to see the project structure.
  </summary>

```bash
â”œâ”€â”€ docker-compose.hasura.yaml
â”œâ”€â”€ engine
â”‚Â Â  â”œâ”€â”€ auth_config.json
â”‚Â Â  â”œâ”€â”€ metadata.json
â”‚Â Â  â””â”€â”€ open_dd.json
â”œâ”€â”€ hasura.yaml
â”œâ”€â”€ supergraph
â”‚Â Â  â”œâ”€â”€ auth-config.hml
â”‚Â Â  â”œâ”€â”€ compatibility-config.hml
â”‚Â Â  â””â”€â”€ graphql-config.hml
â”œâ”€â”€ supergraph.yaml
â””â”€â”€ <first-subgraph>
```

</details>

You now have all the scaffolding necessary to start developing your supergraph by connecting your first data source.

## Connect your database

You can connect both cloud-hosted and local PostgreSQL databases to Hasura DDN. Regardless of where your database is
hosted, it will be connected to your project using a [**data connector**](/connectors/overview.mdx). Together, with the
CLI, the PostgreSQL data connector will generate and use metadata that Hasura can understand to integrate your data
source into your project.

### Step 1. Init the PostgreSQL connector

To initialize the PostgreSQL connector, run the following, replacing `<subgraph-name>` with the name of the subgraph
which will contain this connector and its eventual data source:

```bash
ddn connector init mypg --dir <subgraph-name>/connector/mypg --hub-connector hasura/postgres
```

In this command, we're passing a few important values.

**Connector name**

First, we're naming our connector `mypg`, but you can call it whatever makes sense to your business outcome. For
example, if this connector is integrating a database all about users' profiles, it would make sense to name it
`user_profile_connector` so you and your team know which data source this relates to. **Importantly, a data connector
can only connect to a single data source.**

**Directory for the connector**

Second, we're passing the `--dir` flag to tell the CLI to which subgraph this connector belong _and_ that the
configuration files it's generating should be organized in a `connector/mypg` directory. **Note: the name of the
connector and the directory in which the configuration is stored â€” `mypg` in this example â€”Â should match.**

**Connector type**

Additionally, we're calling out another value: `hasura/postrgres`, which is the connector from the
[Connector Hub](https://hasura.io/connectors/postgres).

### Step 2. Modify the connector's published port

By default, all connectors run on `8080`. However, to avoid port collisions, we'll change the connector's published port
to `8081`. In the `docker-compose.mypg.yaml` file, which can be found in the directory you created in the last step,
replace the existing `published` value with:

```yaml
ports:
  - mode: ingress
    target: 8080
    #highlight-start
    published: "8081"
    #highlight-end
    protocol: tcp
```

### Step 3. Include the PostgreSQL connector's compose file

In our main `docker-compose.yaml` in the root of the project, add the following at the top taking care to replace
`<subgraph-name>` with your subgraph:

```yaml
include:
  - path: <subgraph-name>/connector/mypg/docker-compose.mypg.yaml
```

### Step 4. Add the connection URI

Now that our connector has been scaffolded out for us, we need to provide a connection string so that the data source
can be introspected and the boilerplate configuration can be taken care of by the CLI. Below, note that there are a few
caveats to consider before continuing.

:::info Environment-specific caveats

**Local PostgreSQL**

If you're using a local PostgreSQL database â€” such as through [Docker](https://hub.docker.com/_/postgres) â€” you'll need
to use a tool like [ngrok](https://ngrok.com/) to tunnel your database's connection. This will expose the port, most
likely `5432`, on which the database is running and allow Hasura DDN to connect to it.

**Cloud-hosted PostgreSQL**

Alternatively, if you have a cloud-hosted database, as Hasura DDN will need to reach your database, ensure you've
allowlisted `0.0.0.0/0` so that DDN is able to reach it.

:::

Each connector automatically scaffolds out an env file. We can add a key-value pair of `CONNECTION_URI` along with the
connection string itself to this file and our connector will use this to connect to our PostgreSQL database. The file,
after adding the `CONNECTION_URI` should look like this:

```env
OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://local-dev.hasura.me:4317
OTEL_SERVICE_NAME=mypg
CONNECTION_URI=<postgres-connection-uri>
```

:::tip o11y via OpenTelemetry

Yes! Connectors ship with OTEL-enabled tracing available, out of the box ðŸŽ‰

:::

### Step 5. Introspect your database

With connector configured, we can now use the CLI to introspect our PostgreSQL database create a source-specific
configuration file for our connector:

```bash
ddn connector introspect --connector ux/connector/mypg/connector.yaml
```

If you look at the `configuration.json` for your connector, you'll see metadata describing your PostgreSQL schema.

:::tip Initialize a Git repository

At this point, we recommend initializing a Git repository. This gives you a fallback point as you begin to iterate on
your project.

:::

### Step 6. Create the Hasura metadata

Hasura DDN uses a concept called a [`DataConnectorLink`](/supergraph-modeling/data-connectors.mdx) to take
[NDC-compliant](https://github.com/hasura/ndc-spec) `configuration.json` and transform it into an `hml` (Hasura Metadata
Language) file.

You can create this file, which will be scaffolded out as `mypg.hml` in a new `metadata` subdirectory for your subgraph
by running:

```bash
ddn connector-link add mypg
```

The generated file has two environment variables â€” one for reads and one for writes â€”Â that you'll need to add to your
subgraph's `.env` file. As with the example we've followed so far, if we have a subgraph of `ux`, we'll need to add the
following snippet to our `ux/.env.ux` file:

```env
UX_MYPG_READ_URL=http://local-dev.hasura.me:8081
UX_MYPG_WRITE_URL=http://local-dev.hasura.me:8081
```

These values are for the PostgreSQL connector itself and utilize `local-dev.hasura.me` to ensure proper resolution
within the docker container.

### Step 7. Update the DataConnectorLink

Finally, now that our DataConnectorLink has the correct environment variables configured for the PostgreSQL connector,
we can update it to reflect our database's schema in `hml`:

```bash
ddn connector-link update mypg
```

After this command runs, you can open your `<subgraph>/connector/mypg/metadata/mypg/mypg.hml` file and see your metadata
completely scaffolded out for you ðŸŽ‰

## Basics

### Step 1. Start the services

With all our configuration taken care of, we have a blank slate with which to iterate on our API using this single
source. Let's start developing our API by starting all our services, which includes the GraphQL engine, an
authentication webhook provider, Jaeger for observability, and our connector. Run the following from the root of the
project:

```bash
docker compose -f docker-compose.hasura.yaml watch
```

You should see your various services returned as `Running` or `Started` in the terminal. This provides a watch mode,
which will allow us to iteratively modify and test our API locally with blazing-fast feedback loops ðŸš€

### Step 2. Add a model

Hasura DDN works by creating immutable builds, similar to git commits, based on changes to your project's metadata. This
frees you to iteratively and rapidly test incremental changes to your API without causing disruption to other
collaborators or downstream consumers. Before we create a supergraph build, let's first add a
[**model**](/supergraph-modeling/models.mdx) to our subgraph.

Models serve as the link between your data connector and the API Hasura generates. For PostgreSQL, models typically
correspond to tables, but you can also create them for views and
[native queries](/connectors/postgresql/native-queries/index.mdx).

If you look in your `<subgraph>/connector/mypg/connector/mypg.hml` file, you'll see `object_types` for each table in
your PostgreSQL database. For each of these, you can run the following command to create a model:

```bash
ddn model add --connector-link mypg --name <object_type>
```

:::tip Casing matters

When adding a model, it is critical to use the same casing as the `object_type` from your connector's configuration
file.

:::

<details>
  <summary>
    Each time you add a model, you'll see a new named file in the `models` directory of your connector. For example, if we
    ran the previous command on the `carts` type, we'd then see a `Carts.hml` file that contains all the metadata Hasura DDN
    needs to add it to our API. Click here to see an example `hml` file.
  </summary>

```yaml
# Carts.
---
kind: ObjectType
version: v1
definition:
  name: Carts
  fields:
    - name: createdAt
      type: Timestamptz
    - name: id
      type: Uuid!
    - name: isComplete
      type: Bool!
    - name: isReminderSent
      type: Bool!
    - name: updatedAt
      type: Timestamptz
    - name: userId
      type: Uuid!
  graphql:
    typeName: App_Carts
    inputTypeName: App_CartsInput
  dataConnectorTypeMapping:
    - dataConnectorName: pg_connector
      dataConnectorObjectType: carts
      fieldMapping:
        createdAt:
          column:
            name: created_at
        id:
          column:
            name: id
        isComplete:
          column:
            name: is_complete
        isReminderSent:
          column:
            name: is_reminder_sent
        updatedAt:
          column:
            name: updated_at
        userId:
          column:
            name: user_id

---
kind: TypePermissions
version: v1
definition:
  typeName: Carts
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - id
          - isComplete
          - isReminderSent
          - updatedAt
          - userId

---
kind: ObjectBooleanExpressionType
version: v1
definition:
  name: CartsBoolExp
  objectType: Carts
  dataConnectorName: pg_connector
  dataConnectorObjectType: carts
  comparableFields:
    - fieldName: createdAt
      operators:
        enableAll: true
    - fieldName: id
      operators:
        enableAll: true
    - fieldName: isComplete
      operators:
        enableAll: true
    - fieldName: isReminderSent
      operators:
        enableAll: true
    - fieldName: updatedAt
      operators:
        enableAll: true
    - fieldName: userId
      operators:
        enableAll: true
  graphql:
    typeName: App_CartsBoolExp

---
kind: Model
version: v1
definition:
  name: Carts
  objectType: Carts
  source:
    dataConnectorName: pg_connector
    collection: carts
  filterExpressionType: CartsBoolExp
  orderableFields:
    - fieldName: createdAt
      orderByDirections:
        enableAll: true
    - fieldName: id
      orderByDirections:
        enableAll: true
    - fieldName: isComplete
      orderByDirections:
        enableAll: true
    - fieldName: isReminderSent
      orderByDirections:
        enableAll: true
    - fieldName: updatedAt
      orderByDirections:
        enableAll: true
    - fieldName: userId
      orderByDirections:
        enableAll: true
  graphql:
    selectMany:
      queryRootField: app_carts
    selectUniques:
      - queryRootField: app_cartsById
        uniqueIdentifier:
          - id
    orderByExpressionType: App_CartsOrderBy

---
kind: ModelPermissions
version: v1
definition:
  modelName: Carts
  permissions:
    - role: admin
      select:
        filter: null
```

</details>

### Step 3. Create a supergraph build

Next, we'll create our first supergraph build. We'll pass the `local` subcommand along with specifying the output
directory as `./engine` in the root of the project. This directory is used by the docker-compose file to serve the
engine locally:

```bash
ddn-staging supergraph build local --output-dir ./engine
```

You can now navigate to [`http://localhost:3000`](http://localhost:3000) and interact with your API using the GraphiQL
playground!

<!-- The CLI will respond with information about our project, including a link to the project's console. Click on that to see -->
<!-- the GraphiQL explorer and interact with your API. -->

### Step 4. Write your first query

For the `Carts` example we've been using, we can run the following query:

```graphql
query AllCartsQuery {
  ux_carts {
    id
    isComplete
    userId
  }
}
```

All types are namespaced with the subgraph to which they belong. Here, you can see that `carts` belongs to the `ux`
subgraph. Thus, the query above will return a response that looks like this:

<!-- TODO: Add screenshot of console -->

:::tip Want to import every table and foreign-key relationship?

You can run the following instead of atomically adding models to your API:

```bash
ddn connector-link update mypg --add-all-resources
```

:::

## Relationships

Relationships unlock the true potential of GraphQL. And, with Hasura's
[LSP](https://microsoft.github.io/language-server-protocol/) extension, you can quickly and easily create
[relationships](/supergraph-modeling/relationships.mdx) between entities, allowing you to make deeply nested queries with
whatever access control rules you desire.

### Step 1. Add another model

Begin by adding at least one other model to your metadata. In our example, we'll add the `Users` model so that we can
create a query that returns a user's information and any carts belonging to them:

```bash
ddn model add --connector-link mypg --name users
```

### Step 2. Create the relationship

This will create a new `hml` file for the new model; for our example, we'll create the relationship in the `Users.hml`
file as a user **owns** a cart. To do this, we'll open the `Users.hml` file in VS Code and add the following:

```yaml
---
kind: Relationship
version: v1
definition:
  name: carts
  source: Users
  target:
    model:
      name: Carts
      relationshipType: Array
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        modelField:
          - fieldName: userId
```

:::tip Use the LSP for easy authoring

You can quickly and easily scaffold out a relationship using the Hasura extension, powered by LSP. If you start typing
`relationship` after demarcating the new object with `---`, you'll see the LSP provide you a set of metadata objects
which you can add. After selecting the `to model` relationship option, you can simply tab through the fields and Hasura
will know which values are available to your models.

:::

### Step 3. Create a new supergraph build

As we've changed our metadata, we'll need to create a new build of our supergraph based on those changes:

```bash
ddn supergraph build local --output-dir ./engine
```

### Step 4. Run a nested query

After selecting the most-recent build on your project's console, you can now execute a query using your relationship as a
field on the type in which you created it earlier. As an example, using our data model, we can now query all users'
information **and** their carts:

```graphql
query AllUsersCartsQuery {
  ux_users {
    id
    name
    carts {
      id
      isComplete
      userId
    }
  }
}
```

Which will return information like this:

<!-- TODO: Add screenshot of console -->

## Model evolution

Historically, iterating on APIs has presented challenges. Notably, versioning standards and schema evolution are not
handled elegantly and can lead to downstream disruptions for consumers. With Hasura, iterating on your API is as simple
as updating your metadata. We'll take a look at two common scenarios: adding a new field and deprecating an existing
one.

### Add a new field

As a model is supported by an underlying entity in your PostgreSQL database, you'll need to modify that first. If, for
instance, you add a column â€” such as `last_online` to a table â€” you can then update your `DataConnectorLink` by running
the following:

```bash
ddn connector-link update mypg
```

This will instantly introspect your data source and update the connector's configuration to reflect the new `lastOnline`
field. If this is part of our `Users` model, we'd then run the following to update the metadata:

```bash
ddn model update Users
```

If you have no plans to deprecate an existing field, you can now generate a new supergraph build.

### Deprecate an existing field

With a new field taking its place, we can now deprecate our existing `lastSeen` field by adding the `deprecated` key and
â€” optionally â€” providing a reason that will be passed along during schema introspection.

In the file whose field we want to deprecate, find the field and add the following in the `ObjectType` definition:

```yaml
---
kind: ObjectType
version: v1
definition:
  name: Users
  fields:
    # Other fields above...
    - name: lastSeen
      type: Timestamptz
      # highlight-start
      deprecated:
        reason: Use lastOnline instead.
      # highlight-end
```

If you now create a new build and try to run a query that includes this deprecated field, you'll receive a warning
alerting you to the field's deprecation:

<!-- TODO: Add screenshot -->

## Transformations

As we alluded to in the beginning of this guide, the remaining work on top of your existing PostgreSQL modeling is
authoring and maintaining custom code, or business logic. With Hasura, you can integrate â€” and even host â€” this business
logic directly with Hasura DDN and your API.

Hasura handles business logic using the TypeScript connector. This enables you to author you own custom code, written in
TypeScript, and host it alongside your API. Using this connector, you can transform and enrich data before it reaches
your consumers, unlocking you to simplify client applications and speed up your backend development.

You can then integrate this custom code as individual [**commands**](/supergraph-modeling/commands.mdx) in your
metadata.

### Step 1. Add the TypeScript connector

Let's begin by adding the connector to our project. In the example below, you'll see some familiar flags with new values
being passed to them. We'll call this the `ts_connector` and use the `hasura/nodejs` connector from the connector hub:

```bash
ddn connector init myts --dir <subgraph-name>/connector/myts --hub-connector hasura/nodejs
```

This will create the following directory structure in your subgraph's connector directory's new `myts` directory, with
the `functions.ts` file being your connector's entrypoint:

```bash
â”œâ”€â”€ connector.yaml
â”œâ”€â”€ docker-compose.myts.yaml
#highlight-start
â”œâ”€â”€ functions.ts
#highlight-end
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ package.json
â””â”€â”€ tsconfig.json
```

### Step 2. Modify the connector's published port

Like with our PostgreSQL connector, to avoid port collisions, we'll change the connector's published port to `8082`. In
the `docker-compose.myts.yaml` file, which can be found in the directory you created in the last step, replace the
existing `published` value with:

```yaml
ports:
  - mode: ingress
    target: 8080
    #highlight-start
    published: "8082"
    #highlight-end
    protocol: tcp
```

Then, add the port's value to the `.env/.local` in your `myts` directory:

```env
HASURA_CONNECTOR_PORT=8082
```

### Step 3. Install dependencies

Within our `myts` directory, let's install any necessary dependencies:

```bash
cd <subgraph>/connector/myts && npm i
```

Then, from the `myts` directory run this command to start and watch the connector for any changes:

```bash
env $(cat .env.local | xargs) npm run watch
```

### Step 4. Add the DataConnectorLink

As with our PostgreSQL connector, and any connector, we'll now need to create the `DataConnectorLink` which will
translate our TypeScript functions into commands that can be exposed as queries and mutations via our GraphQL API.
Create this using:

```bash
ddn connector-link add myts
```

Then, update the `.env` values in your subgraph's env file to include this connector. The complete file should now look
like this:

```env
UX_MYPG_READ_URL=http://local-dev.hasura.me:8081
UX_MYPG_WRITE_URL=http://local-dev.hasura.me:8081
UX_MYTS_READ_URL=http://local-dev.hasura.me:8082
UX_MYTS_WRITE_URL=http://local-dev.hasura.me:8082
```

### Step 5. Write business logic

In our example, we're going to transform the timestamp returned on our `createdAt` field from `Users` into a
human-readable format. We'll replace the default `hello()` function in our `functions.ts` file with the following:

```ts
import { Client } from "pg";

/**
 * @readonly
 */
export async function humanReadableCreatedAt(userId: string): Promise<string> {
  const client = new Client({
    host: "35.236.11.122",
    port: 5432,
    database: "v3-docs-sample-app",
    user: "read_only_user",
    password: "readonlyuser",
  });

  await client.connect();

  const queryText = "SELECT created_at FROM users WHERE id = $1";
  const result = await client.query(queryText, [userId]);

  if (result.rows.length > 0) {
    const timestamp = result.rows[0].created_at;
    const date = new Date(timestamp);
    return date.toLocaleString();
  } else {
    return `Issue retrieving user`;
  }
}
```

:::tip Install any dependency

As this is a Node.js project, you can install any dependency! You can see we've installed the `pg` package and are using
it to interact directly with our PostgreSQL data source.

:::

### Step 6. Track the new function

To add our function, similar to how we added our individual tables earlier, we can use the following to generate the
related Hasura metadata:

```bash
ddn connector-link update myts --add-all-resources
```

### Step 7. Create a new build and test

Next, let's create a new build of our supergraph:

```bash
ddn supergraph build local --output-dir ./engine
```

You should see your command available, along with its documentation, in the GraphiQL explorer:

<!-- TODO: Add screenshot of console -->

### Step 8. Create a relationship via a model

As it stands, our `humanReadableCreatedAt` command isn't super useful. It's preferable to return this value when we're
querying for a user's information. We can easily utilize this business logic in a request from a client by creating a
relationship.

For our example, let's take our `Users.hml` file and create a new relationship. Remember, we can use the LSP to help us
in authoring this:

```yaml
---
kind: Relationship
version: v1
definition:
  name: humanReadableCreatedAt
  source: Users
  target:
    command:
      name: HumanReadableCreatedAt
  mapping:
    - source:
        fieldPath:
          - fieldName: id
      target:
        argument:
          argumentName: userId
```

We can then update our `Users` model:

```bash
ddn model update Users
```

And create a new build before testing this:

```bash
ddn-staging supergraph build local --output-dir ./engine
```

In our example, when querying a user's information, we can now return the transformed data as part of the simple query
from the client ðŸŽ‰

<!-- TODO: Add screenshot -->

## Authorization

Powerful authorization rules can be created simply using the same declarative metadata approach we've seen for
relationships and versioning. By adding
[`modelPermissions`](/supergraph-modeling/permissions.mdx#modelpermissions-modelpermissions) and
[`typePermissions`](/supergraph-modeling/permissions.mdx#typepermissions-typepermissions), you can easily control which
rows and columns from PostgreSQL are returned via your API.

### Step 1. Create a modelPermission

By default, a role of `admin` exists for your API and can access all models. To create a new role, such as `user`,
simply add the role to the list of `permissions` for a model and set up your access control rules. In the example below,
we'll allow users with the role of `user` to access only their own rows of data by checking for a header value matching
their `id`:

```yaml
# Users.hml
---
kind: ModelPermissions
version: v1
definition:
  modelName: Users
  permissions:
    - role: admin
      select:
        filter: null
        #highlight-start
    - role: user
      select:
        filter:
          fieldComparison:
            field: id
            operator: _eq
            value:
              sessionVariable: x-hasura-user-id
        #highlight-end
```

### Step 2. Create a typePermission

The `admin` role also has access to all fields for each model. However, adding a new role and limiting what columns can
be returned from your data source is just as simple. For example, let's restrict what a `user` can see by omitting
several fields from the `typePermissions`:

```yaml
---
kind: TypePermissions
version: v1
definition:
  typeName: Users
  permissions:
    - role: admin
      output:
        allowedFields:
          - createdAt
          - email
          - favoriteArtist
          - id
          - isEmailVerified
          - lastSeen
          - name
          - password
          - updatedAt
    #highlight-start
    - role: user
      output:
        allowedFields:
          - email
          - favoriteArtist
          - id
          - name
          - password
    #highlight-end
```

Creating a new build after modifying this metadata will allow us to use two headers in our request: `x-hasura-role` and
`x-hasura-user-id`. In this example, we're passing them as unencoded values, but you can make these part of your claims
when utilizing [JWTs](/auth/authentication/jwt.mdx) or [webhooks](/auth/authentication/webhook.mdx) for authentication:

<!-- TODO: Add screenshot from console -->

:::tip Don't forget about LSP!

Remember, the LSP will help you author any metadata objects or updates, including `modelPermissions` and
`typePermissions`.

:::

## Writes and transactions

You can easily write data to your database using the native PostgreSQL driver and the TypeScript connector. In the
example below, we've truncated the code used in this section, but you follow these steps to create a function which
manipulates or inserts data and is exposed as a mutation via your GraphQL API.

### Step 1. Create a function

In the earlier example, we marked our `humanReadableCreatedAt()` function as `@readOnly` using JSDoc to expose it as a
query. However, omitting this value will expose the function as a mutation. We can even add documentation that will be
visible in the GraphiQL explorer by tagging `@param` for the argument(s) and `@returns` for what will be returned by the
mutation.

```ts
import { Client } from "pg";

/**
 * @param userData An object containing the user's data.
 * @returns The ID of the newly inserted user.
 */
export async function insertUser(userData: { name: string; email: string }): Promise<string> {
  const client = new Client({
    host: "<HOST>",
    port: <PORT>,
    database: "<DB_NAME>",
    user: "<DB_USERNAME>",
    password: "<DB_PASSWORD>",
  });

  await client.connect();

  const queryText = `
    INSERT INTO users (name, email)
    VALUES ($1, $2)
    RETURNING id
  `;
  const values = [userData.name, userData.email];
  const result = await client.query(queryText, values);

  await client.end();

  if (result.rows.length > 0) {
    return result.rows[0].id;
  } else {
    throw new Error("Failed to insert user");
  }
}
```

### Step 2. Track the function

Just as before, we'll track our function using the CLI by first updating our `DataConnectorLink`:

```bash
ddn connector-link update myts
```

And then bringing in the new command:

```bash
ddn command add --connector-link myts --name insertUser
```

### Step 3. Create a new build and test

Next, let's create a new build of our supergraph:

```bash
ddn supergraph build local --output-dir ./engine
```

Finally, should see your command available, along with its documentation, in the GraphiQL explorer as a mutation:

<!-- TODO: Add screenshot of console -->

## Decoupling your domain and physical data models

Native Queries allow you to run custom SQL queries on your PostgreSQL database. This allows you to run queries that are
not supported by Hasura DDN's GraphQL engine. This unlocks the full power of your database, allowing you to run complex
queries, mutations, and even vector searches â€” all directly from your Hasura GraphQL API.

In modern data architecture, decoupling the data domain from physical data models is a critical principle. This approach
promotes flexibility and scalability, allowing the logical data representation to evolve independently of the underlying
physical storage. By using Native Queries, you can define complex business logic and data transformations in SQL while
exposing a clean and simplified GraphQL API. This separation ensures that changes in the database schema or physical
data models do not disrupt the application layer, fostering a more maintainable and adaptable system architecture.

You can read more about these [here](/connectors/postgresql/native-queries/index.mdx).

## Moving to production

At this point, you have all the ingredients and knowledge to create a robust supergraph that composes data across
various sources and aggregates them into a single, reliable, performant API. Before moving to production, consider the
resources below:

### Migrations

Hasura recommends a number of third-party solutions for managing database migrations. Commonly, users implement
migrations via CI/CD with [Flyaway](https://flywaydb.org/) or similar resources.

:::info Doesn't Hasura manage migrations?

In v2, Hasura provided a built-in migration tool. However, as v3 metadata is decoupled from the underlying data source,
you are free to manage your migrations however you wish.

:::

### Performance optimizations

Hasura provides a suite of observability tools directly in a project's DDN console. You can view traces, query plans,
and general usage statistics. These are helpful for diagnosing common bottlenecks and problems with your application's
performance. You can read more about these [here](/observability/overview.mdx).

### CI/CD

You can create a pipeline for deployments using any tools you wish. As we recommend initializing a git repository early
in the project creation process, and provide operability with environment variables, you can follow any git-workflow
best practices for moving between development, staging, and production environments. Additionally, we provide a
configurable [GitHub Action](https://github.com/marketplace/actions/ddn-deployment) for automatically managing your
deployments.
