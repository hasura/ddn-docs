import Thumbnail from "@site/src/components/Thumbnail";

## What's about to happen?

We want to connect our [ClickHouse](https://clickhouse.com/) instance to our API. To do this, we use the Hasura
ClickHouse data connector to facilitate the connection and then introspect the database to generate JSON which the
Hasura CLI will then use to create metadata which can then define your API.

<Thumbnail src="/img/get-started/ERD/connect-data.png" alt="Connect a data source" width="1000px" />

## Step 1. Initialize the ClickHouse connector

:::tip Required

- [The DDN CLI, VS Code extension, and Docker installed](/getting-started/build/00-prerequisites.mdx)
- A new or existing [supergraph](/getting-started/build/01-init-supergraph.mdx)
- A new or existing [subgraph](/getting-started/build/02-init-subgraph.mdx)

:::

To initialize the ClickHouse connector, with the appropriate subgraph set in context, run the following command in your
terminal:

```bash title="Run the following command:"
ddn connector init my_clickhouse -i
```

- Select `hasura/clickhouse` from the list of connectors.
- Choose a port (press enter to accept the default recommended by the CLI).
- Enter your connection details.
- In this example, we've called the connector `my_clickhouse`. You can name it something descriptive.

:::tip Best practices

Importantly, a data connector can only connect to one data source.

The project will be kept organized with each data connector's configuration located in a relevant subgraph directory. In
this example the CLI will create a `my_subgraph/connector/my_clickhouse` directory if it doesn't exist. You can also
change this directory by passing a `--dir` flag to the CLI.

We recommend that the name of the connector and the directory in which the configuration is stored, `my_clickhouse` in
this example, should match for convenience and clarity sake.

:::

### What did `connector init` do?

In the `my_subgraph/connector/my_clickhouse` directory, the CLI created:

- A `connector.yaml` file which contains the local configuration for the connector.
- A `.hasura-connector` folder which contains the connector definition used to build and run it.
- A `compose.yaml` a file to run the ClickHouse data connector locally in Docker.
- A placeholder `.ddnignore` file to prevent unnecessary files from being included in the build.

In the `my_subgraph/metadata` directory, the CLI created:

- A `my_clickhouse.hml` file which contains the [`DataConnectorLink`](/supergraph-modeling/data-connector-links.mdx)
  metadata object which describes how the supergraph can interact with the connector.

Right now, the CLI has only scaffolded out configuration files for the data connector. Our connector still knows nothing
about the ClickHouse database or the data it contains. That's coming up in the next steps.

You can use a local ClickHouse database or a cloud-hosted option. If you already have one you can connect to, you can go
ahead and do that using the steps above. Hasura DDN will not modify your database in any way, so you can use an existing
database without any worries.

:::tip Docker networking Inside a Docker container

`local.hasura.dev` is set to the `host-gateway` alias in the `extra_hosts` option. With this option set,
`local.hasura.dev` resolves to the host machine's gateway IP address from _inside_ the container. This allows various
containers, such as the GraphQL Engine and data connectors, to communicate with each other and out the host machine.

:::

:::info Environment-specific caveats

**Local ClickHouse**

If you're using a local ClickHouse database — such as through
[Docker](https://hub.docker.com/r/clickhouse/clickhouse-server/) — you can connect to it directly from the data
connector. However, if you deploy your connector to Hasura DDN, the cloud-hosted version of your data connector won't be
able to find your database. You'll need to use a tool like [ngrok](https://ngrok.com/) to tunnel your database's
connection. This will expose the port, most likely `9000`, on which the database is running and allow Hasura DDN to
connect to it.

**Cloud-hosted ClickHouse**

Alternatively, if you have a cloud-hosted database, as Hasura DDN will need to reach your database, ensure you've
allowed connections from anywhere (for now) so that DDN is able to reach it.

:::

## Step 2. Introspect your database

With the connector configured, we can now use the CLI to introspect our ClickHouse database. This step will create a
data connector specific configuration file, and generate the necessary Hasura metadata which describes our API by
creating files for each table in our database. These tables will be tracked as
[Models](/supergraph-modeling/models.mdx).

```bash title="Run the following command:"
ddn connector introspect my_clickhouse
```

## What did `connector introspect` do?

The command introspected your data source to create a JSON configuration file.

In your terminal window, the CLI started your connector using its `compose.yaml` and then fetched the schema of your
ClickHouse database.

If you look at the `configuration.json` for your connector, you'll see metadata describing your ClickHouse schema in a
format which the connector specifies.

Additionally, the CLI updated the `DataConnectorLink` object with the latest metadata to interact with the connector.

:::tip Initialize a Git repository

At this point, we recommend initializing a Git repository. This gives you a fallback point as you begin to iterate on
your project.

:::

## Step 3. Track your tables

Tables from ClickHouse are represented as [models](/supergraph-modeling/models.mdx) in your API. The next commands we'll
run will take each table in your database and create an `hml` file for it. These files will then be used by the Hasura
engine to generate your API.

```bash title="Run the following to create your models and relationships:"
ddn model add my_clickhouse "*"
ddn relationship add my_clickhouse "*"
```

If you look in the `metadata` directory for your subgraph, you'll see named files for each resource. These will also
contain relationships based on foreign keys, allowing you to make nested queries in your GraphQL API.

## Step 4. Create a new build and restart the services

To reflect the changes in your API, create a new build.

```bash title= "Run the following:"
ddn supergraph build local
```

And, if your services are not already running, start them.

```bash title="Run the following:"
ddn run docker-start
```

You should see your models available in your API by opening your console using:

```bash title="Run the following:"
ddn console --local
```

## Next steps

With our data source connected and all of our models tracked, we can move on to
[add custom authorization rules](/getting-started/build/05-add-permissions.mdx) using permissions,
[incorporate custom business logic](/getting-started/build/06-add-business-logic.mdx), or
[create relationships](/getting-started/build/07-create-a-relationship.mdx) across data sources!
