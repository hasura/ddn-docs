import Thumbnail from "@site/src/components/Thumbnail";

## What's about to happen?

We want to connect our [MongoDB](https://www.mongodb.com/) database to our API. To do this, we use the Hasura MongoDB
data connector to facilitate the connection and then introspect the database to generate JSON which the Hasura CLI will
then use to create metadata which can then define your API.

<Thumbnail src="/img/get-started/ERD/connect-data.png" alt="Connect a data source" width="1000px" />

## Step 1. Initialize the MongoDB connector

:::tip Required

- [The DDN CLI, VS Code extension, and Docker installed](/getting-started/build/00-prerequisites.mdx)
- A new or existing [supergraph](/getting-started/build/01-init-supergraph.mdx)
- A new or existing [subgraph](/getting-started/build/02-init-subgraph.mdx)

:::

To initialize the MongoDB connector, run the following in your terminal:

```bash title="Run the following command:"
ddn connector init -i
```

- Select `hasura/mongodb` from the list of connectors.
- Name it something descriptive. For this example, we'll call it `my_mongo`.
- Choose a port (press enter to accept the default recommended by the CLI).
- Enter your connection string

:::tip Best practices

Importantly, a data connector can only connect to one data source.

The project will be kept organized with each data connector's configuration located in a relevant subgraph directory. In
this example the CLI will create a `my_subgraph/connector/my_mongo` directory if it doesn't exist. You can also change
this directory by passing a `--dir` flag to the CLI.

We recommend that the name of the connector and the directory in which the configuration is stored, `my_mongo` in this
example, should match for convenience and clarity sake.

In subsequent steps, when running your connector locally, it's critical to ensure the port value matches the connection
string you provide in the key-value pair for your `.env` file.

:::

### What did `connector init` do?

In the `my_subgraph/connector/my_mongo` directory which we specified in the command, the CLI created:

- A `connector.yaml` file which contains the local configuration for the connector.
- A `.hasura-connector` folder which contains the connector definition used to build and run it.
- A `compose.yaml` a file to run the MongoDB data connector locally in Docker.
- A placeholder `.ddnignore` file to prevent unnecessary files from being included in the build.

In the `my_subgraph/metadata` directory, the CLI created:

- A `my_mongo.hml` file which contains the [`DataConnectorLink`](/supergraph-modeling/data-connector-links.mdx)
  metadata object which describes how the supergraph can interact with the connector.

Right now, the CLI has only scaffolded out configuration files for the data connector. Our connector still knows nothing
about the MongoDB database or the data it contains. That's coming up in the next steps.

:::tip Need a connection string?

Feel free to use this read-only MongoDB connection string for testing purposes:

```text
mongodb+srv://read_only_user:readonlyuser@v3-docs-sample-app.vh2tp.mongodb.net/sample_mflix?retryWrites=true&w=majority&appName=v3-docs-sample-app
```

:::

Remember to use quotes as the string will likely have some characters which will break the `.env` file if not enclosed.

You can use a local MongoDB database, or a cloud-hosted one like MongoDB Atlas. Check out
[this page](https://www.mongodb.com/docs/manual/installation/) for a list of options for running a MongoDB database if
you don't already have one. If you already have one you can connect to, you can go ahead and do that. Hasura DDN will
not modify your database in any way, so you can use an existing database without any worries.

:::tip Docker networking Inside a Docker container

`local.hasura.dev` is set to the `host-gateway` alias in the `extra_hosts` option. With this option set,
`local.hasura.dev` resolves to the host machine's gateway IP address from _inside_ the container. This allows various
containers, such as the GraphQL Engine and data connectors, to communicate with each other and out the host machine.

:::

:::info Environment-specific caveats

**Local Mongo**

If you're using a local MongoDB database â€” such as through [Docker](https://hub.docker.com/_/mongodb) â€” you can connect
to it directly from the data connector. However, if you deploy your supergraph to Hasura DDN the cloud-hosted version of
your data connector won't be able to find your database. So tunneling that connection from the start with a tool like
[ngrok](https://ngrok.com/) is a good idea.

**Cloud-hosted MongoDB**

Alternatively, if you have a cloud-hosted database, perhaps with
[MongoDB Atlas](https://www.mongodb.com/products/platform/atlas-database) as Hasura DDN will need to reach your
database, ensure you've allowlisted `0.0.0.0/0` (for now) so that DDN is able to reach it. To learn how to deploy a
MongoDB Atlas cluster, see the [official documentation](https://www.mongodb.com/docs/atlas/getting-started/).

:::

## Step 2. Introspect your database

With the connector configured, we can now use the CLI to introspect our MongoDB database. This step will create a data
connector specific configuration files, and generate the necessary Hasura metadata which describes our API by creating
files for each collection in our database. These collections will be tracked as
[Models](/supergraph-modeling/models.mdx).

```bash title="Run the following command:"
ddn connector introspect my_mongo
```

## What did `connector introspect` do?

The command introspected your data source to create configuration files.

In your terminal window, the CLI started your connector using its `compose.yaml` and then fetched the schema of your
MongoDB database.

Additionally, the CLI updated the `DataConnectorLink` object with the latest metadata to interact with the connector.

These include a `configuration.json` file and a new `schema` directory with a definition of all collections found in
MongoDB in a JSON [NDC-compliant](https://github.com/hasura/ndc-spec) format which the connector specifies.

Eg:

```text
.
â”œâ”€â”€ comments.json
â”œâ”€â”€ embedded_movies.json
â”œâ”€â”€ movies.json
â”œâ”€â”€ sessions.json
â”œâ”€â”€ theaters.json
â””â”€â”€ users.json
```

:::tip o11y via OpenTelemetry

Yes! Connectors ship with OTEL-enabled tracing available, out of the box ðŸŽ‰

:::

## Step 3. Track your collections

Collections from MongoDB are represented as [Models](/supergraph-modeling/models.mdx) and
[Commands](/supergraph-modeling/commands.mdx) in your API. The next command we'll run will take each collection in your
database and create an `hml` file for it. These files will then be used by the Hasura engine to generate your API.

```bash title="Run the following to create your models and commands:"
ddn connector-link add-resources my_mongo
```

If you look in the `metadata` directory for your subgraph, you'll see named files for each resource. These will also
contain relationships based on foreign keys, allowing you to make nested queries in your GraphQL API.

## Step 4. Create a new build and restart the services

To reflect the changes in your API, create a new build.

```bash title= "Run the following:"
ddn supergraph build local
```

And, if your services are not already running, start them.

```bash title="Run the following:"
ddn run docker-start
```

You should see your models available in your API by opening your console using `ddn console --local` ðŸŽ‰

## Next steps

With our data source connected and all of our models tracked, we can move on to
[add custom authorization rules](/getting-started/build/05-add-permissions.mdx) using permissions,
[incorporate custom business logic](/getting-started/build/06-add-business-logic.mdx), or
[create relationships](/getting-started/build/07-create-a-relationship.mdx) across data sources!
