import Thumbnail from "@site/src/components/Thumbnail";

## What's about to happen?

We want to connect our [PostgreSQL](https://www.postgresql.org/) database to our API. To do this, we use the Hasura
PostgreSQL data connector to facilitate the connection and then introspect the database to generate JSON which the
Hasura CLI will then use to create metadata which can then define your API.

<Thumbnail src="/img/get-started/ERD/connect-data.png" alt="Connect a data source" width="1000px" />

## Step 1. Initialize the PostgreSQL connector

:::tip Required

- [The DDN CLI, VS Code extension, and Docker installed](/getting-started/build/00-prerequisites.mdx)
- A new or existing [supergraph](/getting-started/build/01-init-supergraph.mdx)
- A new or existing [subgraph](/getting-started/build/02-init-subgraph.mdx)

:::

To initialize the PostgreSQL connector, with the appropriate subgraph set in context, run the following command in your
terminal.

```bash title="Run the following command:"
ddn connector init -i
```

- Select `hasura/postgres` from the list of connectors.
- Name it something descriptive. For this example, we'll call it `my_pg`.
- Choose a port (press enter to accept the default recommended by the CLI).
- Enter your connection string (press enter to accept the sample, read-only database).

:::tip Best practices

Importantly, a data connector can only connect to one data source.

The project will be kept organized with each data connector's configuration located in a relevant subgraph directory. In
this example the CLI will create a `my_subgraph/connector/my_pg` directory if it doesn't exist.

We recommend that the name of the connector and the directory in which the configuration is stored, `my_pg` in this
example, should match for convenience and clarity sake for this tutorial, but it can be anything you want.

:::

### What did `connector init` do?

In the `my_subgraph/connector/my_pg` directory which we specified in the command, the CLI created:

- A `connector.yaml` file which contains the local configuration for the connector.
- A `.hasura-connector` folder which contains the connector definition used to build and run it.
- A `compose.yaml` a file to run the PostgreSQL data connector locally in Docker.
- A placeholder `.ddnignore` file to prevent unnecessary files from being included in the build.
- A `configuration.json` file which contains configuration for the data connector itself.
- A `schema.json` file which is the JSON schema that `configuration.json` follows. This is to enable autocomplete in VS
  Code.

In the `my_subgraph/metadata` directory, the CLI created:

- A `my_pg.hml` file which contains the [`DataConnectorLink`](/supergraph-modeling/data-connector-links.mdx) metadata
  object which describes how the supergraph can interact with the connector.

Right now, the CLI only scaffolded out configuration files for the data connector. Our connector still knows nothing
about the PostgreSQL database or the data it contains. That's coming up in the next steps.

## Step 2. Update the connection URI

The PostgreSQL connector ships with a sample read-only database connection as the default `CONNECTION_URI`.

In the root of our project, our `.env` was updated by the CLI to include a new set of values, including the connection
URI. **If you wish to override the default demo connection, you can update the key-value pair of `CONNECTION_URI` with
your custom connection string in this file.**

```env title="The .env in the root of our project:"
APP_MY_PG_AUTHORIZATION_HEADER="Bearer <sample token>"
#highlight-start
APP_MY_PG_CONNECTION_URI="postgresql://read_only_user:readonlyuser@35.236.11.122:5432/v3-docs-sample-app"
#highlight-end
APP_MY_PG_HASURA_SERVICE_TOKEN_SECRET="<sample token>"
APP_MY_PG_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT="http://local.hasura.dev:4317"
APP_MY_PG_OTEL_SERVICE_NAME="app_my_pg"
APP_MY_PG_READ_URL="http://local.hasura.dev:8082"
APP_MY_PG_WRITE_URL="http://local.hasura.dev:8082"
```

You can use a local PostgreSQL database or a cloud-hosted option. Hasura DDN will not modify your database in any way,
so you can use an existing database without any worries.

**Don't have a database at hand?** Check [this guide](/connectors/postgresql/local-postgres.mdx) to set up a local
Postgres database using Docker.

:::tip Docker networking Inside a Docker container

`local.hasura.dev` is set to the `host-gateway` alias in the `extra_hosts` option. With this option set,
`local.hasura.dev` resolves to the host machine's gateway IP address from _inside_ the container. This allows various
containers, such as the GraphQL Engine and data connectors, to communicate with each other and out the host machine.

:::

:::info Environment-specific caveats

**Local PostgreSQL**

If you're using a local PostgreSQL database — such as through [Docker](https://hub.docker.com/_/postgres) — you can
connect to it directly from the data connector running locally. However, if you deploy your connector to Hasura DDN, the
cloud-hosted version of your data connector won't be able to find your local database. You'll need to use a tool like
[ngrok](https://ngrok.com/) to provide a tunnel to access your database from the cloud. This will expose the port, most
likely `5432`, on which the database is running and allow Hasura DDN to connect to it.

**Cloud-hosted PostgreSQL**

Alternatively, if you have a cloud-hosted database, as Hasura DDN will need to reach your database, ensure you've
allowlisted `0.0.0.0/0` (for now) so that DDN is able to reach it.

:::

## Step 3. Introspect your database

With the connector configured, we can now use the CLI to introspect our PostgreSQL database. This step will create a
data connector specific configuration file, and generate the necessary Hasura metadata which describes our API by
creating files for each table in our database. These tables will be tracked as
[Models](/supergraph-modeling/models.mdx).

```bash title="Run the following command:"
ddn connector introspect my_pg
```

## What did `connector introspect` do?

The command introspected your data source to create a JSON configuration file.

In your terminal window, the CLI started your connector using its `compose.yaml` and then fetched the schema of your
PostgreSQL database.

If you look at the `configuration.json` for your connector, you'll see metadata describing your PostgreSQL schema in a
format which the connector specifies.

Additionally, the CLI updated the `DataConnectorLink` object with the latest metadata to interact with the connector.

:::tip Initialize a Git repository

At this point, we recommend initializing a Git repository. This gives you a fallback point as you begin to iterate on
your project.

:::

## Step 4. Track your tables

Tables from PostgreSQL are represented as [models](/supergraph-modeling/models.mdx) in your API. The next commands we'll
run will take each table or view in your database and create an `hml` file for it. These files will then be used by the
Hasura engine to generate your API.

```bash title="Run the following to create your models and relationships:"
ddn model add my_pg "*"
ddn relationship add my_pg "*"
```

If you look in the `metadata` directory for your subgraph, you'll see named files for each resource. These will also
contain relationships based on foreign keys, allowing you to make nested queries in your GraphQL API.

## Step 5. Create a new build and restart the services

To reflect the changes in your API, create a new build.

```bash title= "Run the following:"
ddn supergraph build local
```

And, if your services are not already running, start them.

```bash title="Run the following:"
ddn run docker-start
```

You should see your models available in your API by opening your console using:

```bash title="Run the following:"
ddn console --local
```

## Next steps

With our data source connected and all of our models tracked, we can move on to
[add custom authorization rules](/getting-started/build/05-add-permissions.mdx) using permissions,
[incorporate custom business logic](/getting-started/build/06-add-business-logic.mdx), or
[create relationships](/getting-started/build/07-create-a-relationship.mdx) across data sources!

Additionally, if your curious how to update your API after the underlying data source schema changes, check out the next
page on [updating data source metadata](/getting-started/build/03-connect-to-data/02-create-source-metadata.mdx).
