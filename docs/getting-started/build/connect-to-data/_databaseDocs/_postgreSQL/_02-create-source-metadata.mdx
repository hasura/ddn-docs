## What's about to happen?

After initializing a connector and creating a JSON configuration, which represents our tables in PostgreSQL, we can use
it to generate Hasura metadata. In the steps below, we'll utilize our `configuration.json` â€” populated by introspecting
our PostgreSQL database â€” to create metadata which Hasura can use to construct our API.

## Step 1. Create the Hasura metadata

:::tip Required

- [The DDN CLI, VS Code extension, and Docker installed](/getting-started/prerequisites)
- A new or existing [supergraph](/getting-started/init-supergraph)
- A new or existing [subgraph](/getting-started/init-subgraph)
- A [PostgreSQL connector](/getting-started/connect-to-data/connect-a-source) initialized

:::

Hasura DDN uses a concept called "connector linking" to take [NDC-compliant](https://github.com/hasura/ndc-spec)
configuration JSON files for a data connector and transform them into an `hml` (Hasura Metadata Language) file as a
[DataConnectorLink](/supergraph-modeling/data-connector-links/) metadata object.

:::info JSON and HML, why both?

Basically, metadata objects in `hml` files contain the data connector schema in a
[format](https://github.com/hasura/ndc-spec) which is generic for all connectors. The JSON on the other hand contains a
format specific to that connector. It does not need to be JSON, it could just as easily be SQL files, TypeScript
functions, or anything else. It's just a representation that the specific connector understands.

First we generate the schema representation using the `ddn connector introspect` command, and then use that to create
the `DataConnectorLink` hml object with the `ddn connector-link add` and `update` commands.

:::

First we create this `hml` file with the `connector-link add` command and then convert our configuration files into
`hml` syntax and add it to this file with the `connector-link update` command.

Let's name the `hml` file the same as our connector, `my_pg`:

```bash title="Run the following from the root of your project:"
ddn connector-link add my_pg \
  --subgraph my_subgraph/subgraph.yaml \
  --configure-host http://local.hasura.dev:8082 \
  --target-env-file my_subgraph/.env.my_subgraph.local
```

The new file is scaffolded out at `my_subgraph/metadata/my_pg.hml`.

<details>
  <summary>Click here for an example `hml` `DataConnectorLink` file</summary>

```yaml title="my_pg.hml"
kind: DataConnectorLink
version: v1
definition:
  name: my_pg
  url:
    readWriteUrls:
      read:
        valueFromEnv: MY_SUBGRAPH_MY_PG_READ_URL
      write:
        valueFromEnv: MY_SUBGRAPH_MY_PG_WRITE_URL
  schema:
    version: v0.1
    schema:
      scalar_types: {}
      object_types: {}
      collections: []
      functions: []
      procedures: []
    capabilities:
      version: ""
      capabilities:
        query: {}
        mutation: {}
```

</details>

The generated file has two environment variables â€” one for reads and one for writes. Because we used the convenience
flag: `--configure-host` on the command, these values are already set in the `.env.my_subgraph.local` file:

```env title="my_subgraph/.env.my_subgraph"
MY_SUBGRAPH_MY_PG_READ_URL="http://local.hasura.dev:8082"
MY_SUBGRAPH_MY_PG_WRITE_URL="http://local.hasura.dev:8082"
```

These values are for the PostgreSQL connector itself and utilize `local.hasura.dev` to ensure proper resolution within
the docker container.

## Step 2. Start the services

Let's start our docker compose services.

```bash title="From the root of your project, run:"
HASURA_DDN_PAT=$(ddn auth print-pat) docker compose up --build --watch
```

This starts our Hasura Engine, observability tools and the PostgreSQL connector all together since we added the
connector's Docker compose file to the main `compose.yaml` file using the `--add-to-compose-file` flag
when we initialized the connector. `HASURA_DDN_PAT=$(ddn auth print-pat)` gives the Hasura Engine access to the DDN
CLI's authentication token.

We can navigate to the following address, with the port modified, to see the schema of our PostgreSQL database:

```text
http://localhost:8082/schema
```

## Step 3. Update the new DataConnectorLink object with metadata for your PostgreSQL database

Finally, now that our `DataConnectorLink` has the correct environment variables configured for the PostgreSQL connector,
we can run the `update` command to have the CLI look at the configuration JSON and transform it to reflect our
database's schema in `hml` format. In a new terminal tab, run:

```bash title="From the root of your project, run:"
ddn connector-link update my_pg \
  --subgraph my_subgraph/subgraph.yaml \
  --env-file my_subgraph/.env.my_subgraph.local
```

After this command runs, you can open your `my_subgraph/metadata/my_pg.hml` file and see your metadata completely
scaffolded out for you ðŸŽ‰

## What did this do?

By creating a `my_pg.hml` file, we've provided Hasura with a link between our original data source and the types which
we'll eventually expose via our API.

## Next steps

With a data connector fully configured, you can now start to create metadata for each table and view in your PostgreSQL
database using `hml`. Learn how to do this by
[exposing source entities](/getting-started/build/connect-to-data/add-source-entities).
