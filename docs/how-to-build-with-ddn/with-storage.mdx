---
sidebar_position: 8
sidebar_label: With Storage
description: "Learn the basics of Hasura DDN and how to get started with various storage solutions."
keywords:
  - hasura ddn
  - graphql api
  - getting started
  - guide
  - storage
  - bucket
  - s3
---

import Prereqs from "@site/docs/_prereqs.mdx";

# Get Started with Hasura DDN and Cloud Storage

## Overview

This tutorial takes about twenty minutes to complete. You'll learn how to:

- Set up a new Hasura DDN project
- Connect it to a cloud storage instance
- Generate Hasura metadata
- Create a build
- Run your first query
- Mutate data

Additionally, we'll familiarize you with the steps and workflows necessary to iterate on your API.

:::info Available Cloud Storage

In this tutorial, we'll use the Storage connector's local file system, but you can easily configure the connector to
work with:

- [AWS S3](https://aws.amazon.com/pm/serv-s3)
- [Google Cloud Storage](https://cloud.google.com/storage)
- [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs)
- [Cloudflare R2](https://www.cloudflare.com/developer-platform/products/r2/)
- [DigitalOcean Spaces](https://www.digitalocean.com/products/spaces)

Hasura will never modify your source schema. Learn more about these sources in the connector's configuration section.

:::

<Prereqs />

## Tutorial

### Step 1. Authenticate your CLI

```sh title="Before you can create a new Hasura DDN project, you need to authenticate your CLI:"
ddn auth login
```

This will launch a browser window prompting you to log in or sign up for Hasura DDN. After you log in, the CLI will
acknowledge your login, giving you access to Hasura Cloud resources.

### Step 2. Scaffold out a new local project

```sh title="Next, create a new local project:"
ddn supergraph init my-project && cd my-project
```

Once you move into this directory, you'll see your project scaffolded out for you. You can view the structure by either
running `ls` in your terminal, or by opening the directory in your preferred editor.

### Step 3. Initialize your Storage connector

```sh title="In your project directory, run:"
ddn connector init my_storage -i
```

Select `hasura/storage` from the list of connectors. You can start typing `storage` to quickly filter the list; hit
`ENTER` to accept any default values for which you're prompted.

### Step 4. Update your `configuration.yaml`

```yaml title="Update the app/connector/my_storage/configuration.yaml to utilize the local file system by replacing its contents with:"
# yaml-language-server: $schema=https://raw.githubusercontent.com/hasura/ndc-storage/main/jsonschema/configuration.schema.json
clients:
  - id: fs
    type: fs
    defaultDirectory:
      value: /home/nonroot/data
concurrency:
  query: 5
  mutation: 1
runtime:
  maxDownloadSizeMBs: 20
  maxUploadSizeMBs: 20
generator:
  promptqlCompatible: false
```

:::info Navigating the file system

"Local" in this case refers to the container in which the connector is running. The files we create and query will be
ephemeral and unavailable after the container is stopped.

:::

### Step 5. Introspect your cloud storage

```sh title="Next, use the CLI to introspect your cloud storage:"
ddn connector introspect my_storage
```

As the schema for this connector is mapped to file system conventions for S3-compatible providers, the connector does
not generate any local configuration files. Instead, the introspection command above will generate a `my_storage.hml`
file that contains all the [models](/reference/metadata-reference/models.mdx) and
[commands](/reference/metadata-reference/commands.mdx) necessary to interact with your file system via the exposed
GraphQL API.

```sh title="You can check which resources are available â€”Â and their status â€” at any point using the CLI:"
ddn connector show-resources my_storage
```

### Step 6. Add your models

The connector contains two models: `storage_buckets` and `storage_objects`.

```sh title="Track them both with this command:"
ddn model add my_storage "*"
```

Open the `app/metadata` directory and you'll find newly-generated file for both of these models. The DDN CLI will use
these Hasura Metadata Language files to represent the available bucket(s) and objects within it.

### Step 7. Create a new build

```sh title="To create a local build, run:"
ddn supergraph build local
```

The build is stored as a set of JSON files in `engine/build`.

### Step 8. Start your local services

```sh title="Start your local Hasura DDN Engine and Storage connector:"
ddn run docker-start
```

Your terminal will be taken over by logs for the different services.

### Step 9. Run your first query

```sh title="In a new terminal tab, open your local console:"
ddn console --local
```

```graphql title="In the GraphiQL explorer of the console, write this query:"
query GET_ALL_BUCKETS {
  storageBuckets(args: {}) {
    name
  }
}
```

```json title="You'll get the following response:"
{
  "data": {
    "storageBuckets": [
      {
        "name": "/home/nonroot/data"
      }
    ]
  }
}
```

This is the same directory you provided in your `configuration.yaml` file and where we'll soon write new files to via
the API.

### Step 10. Add your commands

Let's write a text file to this directory (bucket). To do so, we can use one of the pre-existing commands in our
`my_storage.hml` file.

```sh title="Run the following to add the UploadStorageObjectAsText as command:"
ddn command add my_storage "upload_storage_object_as_text"
```

We'll also need another command that allows us to download the file as text.

```sh title="We can add that using the following:"
ddn command add my_storage "download_storage_object_as_text"
```

As before, these commands will generate new HML files in the `app/metadata` directory.

### Step 11. Create a new build and restart your services

```sh title="Create the new build:"
ddn supergraph build local
```

```sh title="Kill your locally running services with CTRL+C and then restart them with:"
ddn run docker-start
```

### Step 12. Create a new text file via a mutation

```graphql title="From the GraphiQL explorer in your console, execute the following mutation:"
mutation ADD_TXT_FILE {
  uploadStorageObjectAsText(data: "This is a sample text file.", name: "sample.txt", bucket: "/home/nonroot/data") {
    name
    size
  }
}
```

```json title="Which will return a value like this:"
{
  "data": {
    "uploadStorageObjectAsText": {
      "name": "sample.txt",
      "size": 27
    }
  }
}
```

### Step 13. Query the new file's contents

```graphql title="You can now query that file's text value:"
query GET_TEXT_VALUE_FROM_OBJECT {
  downloadStorageObjectAsText(name: "sample.txt") {
    data
  }
}
```

```json title="You'll get a response like this:"
{
  "data": {
    "downloadStorageObjectAsText": {
      "data": "This is a sample text file."
    }
  }
}
```

## Next steps

Congratulations on completing your first Hasura DDN project to interact with cloud storage! ðŸŽ‰

Here's what you just accomplished:

- You started with a fresh project and connected it to the Storage connector.
- You set up metadata to represent your file system and methods of interacting with it, which acts as the blueprint for
  your API.
- Then, you created a build â€” essentially compiling everything into a ready-to-use API â€” and successfully ran your first
  GraphQL queries to fetch data.
- You utilized commands to write a file and then access its text value.
- Along the way, you learned how to iterate on your schema and refresh your metadata to reflect changes.

Now, you're equipped to connect and expose your data, empowering you to iterate and scale with confidence. Great work!

Take a look at our Storage connector docs to learn more about how to use Hasura DDN with cloud storage providers. Or, if
you're ready, get started with adding [permissions](/reference/metadata-reference/permissions.mdx) to control access to
your API.
